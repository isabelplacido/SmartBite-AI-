<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>SmartBite AI – Results & Takeaways</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
<header>
    <h1>SmartBite AI</h1>
    <nav>
        <a href="index.html">Problem & Motivation</a>
        <a href="approach.html">Technical Approach</a>
        <a href="results.html">Results & Takeaways</a>
    </nav>
</header>

<main>
    <!-- VISUALIZATIONS -->
    <section>
        <h2>Visualizations</h2>
        <p>
            The figures below summarize how the models trained and how different experiment
            settings affected performance on the three classes (fresh, mixed, ultraprocessed).
        </p>

        <figure>
            <img src="epoch.png" alt="Epoch progression over training steps">
            <figcaption>
                Overall training progress in terms of epoch vs step for the AlexNet baseline.
            </figcaption>
        </figure>

        <figure>
            <img src="best_validation_accuracy.png"
                 alt="Bar chart of best validation accuracy for each experiment run">
            <figcaption>
                Best validation accuracy achieved by each configuration:
                different batch sizes, learning rates, augmentation settings, and architectures
                (AlexNet vs ResNet18).
            </figcaption>
        </figure>

        <figure>
            <img src="confusion_matrixz_curve.png"
                 alt="Multi-panel summary of metrics for each experiment">
            <figcaption>
                Multi-panel summary of metrics across runs. Each panel represents a specific
                experiment setting and shows how its tracked metrics behaved.
            </figcaption>
        </figure>
    </section>

    <!-- NUMERICAL RESULTS -->
    <section>
        <h2>Results</h2>

        <h3>Batch Size Experiment</h3>
        <p>
            I compared two batch sizes using otherwise similar settings:
        </p>
        <ul>
            <li><strong>bs16:</strong> best validation accuracy ≈ <strong>30.4%</strong></li>
            <li><strong>bs64:</strong> best validation accuracy ≈ <strong>69.6%</strong></li>
        </ul>
        <p>
            A batch size of 64 performed much better, suggesting more stable gradients and better
            use of the small dataset compared to a very small batch of 16.
        </p>

        <h3>Learning Rate Experiment</h3>
        <p>
            I then varied the learning rate:
        </p>
        <ul>
            <li><strong>lr = 1e-3:</strong> best validation accuracy ≈ <strong>73.9%</strong></li>
            <li><strong>lr = 1e-4:</strong> best validation accuracy ≈ <strong>82.6%</strong></li>
        </ul>
        <p>
            The smaller learning rate (1e-4) gave the best overall accuracy, likely because it allowed
            smoother fine-tuning of the pretrained weights without overshooting.
        </p>

        <h3>Data Augmentation Experiment</h3>
        <p>
            I also compared runs with and without basic augmentation:
        </p>
        <ul>
            <li><strong>No augmentation:</strong> best validation accuracy ≈ <strong>73.9%</strong></li>
            <li><strong>With augmentation:</strong> best validation accuracy ≈ <strong>52.2%</strong></li>
        </ul>
        <p>
            On this tiny dataset, augmentation actually hurt performance. The extra variation probably
            made the task harder for the model instead of improving generalization.
        </p>

        <h3>Architecture Comparison</h3>
        <p>
            Finally, I compared AlexNet to a more modern residual network:
        </p>
        <ul>
            <li><strong>AlexNet:</strong> best validation accuracy ≈ <strong>60.9%</strong></li>
            <li><strong>ResNet18:</strong> best validation accuracy ≈ <strong>91.3%</strong></li>
        </ul>
        <p>
            ResNet18 clearly outperformed AlexNet on this three-class food classification task. Even with
            limited data, the residual architecture extracted better features and reached around 91.3%
            validation accuracy.
        </p>
    </section>

    <!-- TAKEAWAYS -->
    <section>
        <h2>Takeaways</h2>
        <ul>
            <li>
                Deep learning models can learn meaningful patterns for distinguishing fresh, mixed,
                and ultraprocessed foods even from a small custom dataset.
            </li>
            <li>
                Hyperparameters matter: batch size 64 and learning rate 1e-4 significantly improved
                performance compared to smaller batch size and a larger learning rate.
            </li>
            <li>
                Simple data augmentation was not helpful here, showing that augmentation can hurt
                if the dataset is tiny and the task is already noisy.
            </li>
            <li>
                ResNet18 strongly outperformed AlexNet (≈91.3% vs ≈60.9% accuracy), reinforcing how much
                architecture choice impacts performance.
            </li>
            <li>
                Because the dataset is small and the labels are broad (fresh / mixed / ultraprocessed),
                this system is a proof-of-concept rather than a reliable health tool.
            </li>
        </ul>
        <p>
            Future work would focus on collecting more data, refining labels, combining images with
            nutrition text, and exploring newer architectures to move closer to something that could
            be used in real-world settings.
        </p>
    </section>
</main>

<footer>
    <p>SmartBite AI · Results & Reflections</p>
</footer>
</body>
</html>
